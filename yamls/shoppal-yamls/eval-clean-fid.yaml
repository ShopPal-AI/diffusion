image_size: 256 # This is the image resolution to evaluate at (assumes square images)
batch_size: 16
name: clean-fid-eval # Name for the eval run for logging
project: diffusion-clean-fid-eval # Name of the wandb project for logging
seed: 42 # Random seed. This affects the randomness used in image generation.

model: # This is the model to evaluate
  _target_: diffusion.models.models.stable_diffusion_2
  pretrained: false
  precomputed_latents: false
  encode_latents_in_fp16: true
  fsdp: false
  val_metrics:
    - _target_: torchmetrics.MeanSquaredError
  val_guidance_scales: []
  loss_bins: []
eval_dataloader:
  _target_: diffusion.datasets.coco.coco_captions.build_streaming_cocoval_dataloader
  remote: # Remote(s) for the evaluation dataset go here
    /data0/shoppal/nixingliang/coco/
  local: # Local(s) for the evaluation dataset go here
    /tmp/eval-fid
  batch_size: ${batch_size}
  resize_size: ${image_size}
  prefetch_factor: 2
  num_workers: 8
  persistent_workers: true
  pin_memory: true
clip_metric: # This is the metric used to compute CLIP score, which is not part of clean-fid
  _target_: torchmetrics.multimodal.CLIPScore
  model_name_or_path: openai/clip-vit-base-patch16
logger:
  wandb:
    _target_: composer.loggers.wandb_logger.WandBLogger
    name: ${name}
    project: ${project}
    group: ${name}
evaluator:
  _target_: diffusion.evaluation.clean_fid_eval.CleanFIDEvaluator
  load_path: # Path to the checkpoint to load and evaluate.
  guidance_scales:
    - 1.0
    - 1.5
    - 2.0
    - 3.0
    - 4.0
    - 5.0
    - 6.0
    - 7.0
    - 8.0
  size: ${image_size}
  batch_size: ${batch_size}
  seed: ${seed}
